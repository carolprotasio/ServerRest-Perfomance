# ServeRest-Performance

## 1. Sobre o Projeto

Este projeto foi desenvolvido como parte dos meus estudos de QA e teste de performance, utilizando o JMeter, Docker, e a biblioteca Faker para gera√ß√£o de dados din√¢micos. O objetivo principal foi realizar testes de performance em diferentes m√≥dulos da API ServeRest (https://serverest.dev/), simulando o comportamento de m√∫ltiplos usu√°rios acessando e manipulando recursos simultaneamente.

Os testes foram realizados nos endpoints que cobrem as funcionalidades de usu√°rio, produto e carrinho, incluindo opera√ß√µes de CRUD (Criar, Ler, Atualizar, Deletar). Esse projeto permitiu aprofundar meus conhecimentos em automa√ß√£o de testes de performance e integra√ß√£o de ferramentas essenciais para a √°rea de QA.

<img src="https://github.com/carolprotasio/ServerRest-Perfomance/blob/master/swagger.png" alt="Swagger" width="600"/>

### 1.1 JMeter

O JMeter √© uma ferramenta de c√≥digo aberto utilizada para testar a performance de aplica√ß√µes web. No contexto deste projeto, o JMeter foi empregado para simular m√∫ltiplos usu√°rios interagindo com a API do ServeRest, permitindo a an√°lise do desempenho da aplica√ß√£o sob diferentes cargas de trabalho.

<img src="https://github.com/carolprotasio/ServerRest-Perfomance/blob/master/jmeter.png" alt="jmeter" width="600"/>

### 1.2 Docker

O Docker foi utilizado para configurar e isolar o ambiente de testes, garantindo que o JMeter e todas as depend√™ncias necess√°rias estivessem corretamente configurados e reproduz√≠veis em qualquer ambiente. Com Docker, foi poss√≠vel garantir consist√™ncia e simplicidade na execu√ß√£o dos testes.

<img src="https://github.com/carolprotasio/ServerRest-Perfomance/blob/master/docker.png" alt="docker" width="600"/>

## 2. Tecnologias Utilizadas

- **JMeter**: Ferramenta para testes de carga e performance.
- **Docker**: Plataforma para virtualiza√ß√£o e automa√ß√£o de ambientes.
- **Faker**: Biblioteca para gera√ß√£o de dados fict√≠cios, usada na cria√ß√£o de cen√°rios de teste.
- **JSR223 Sampler**: Componente do JMeter usado para scripting avan√ßado, onde foi integrado o Faker.

## 3. Funcionalidades e M√≥dulos Testados

Foram testados os seguintes m√≥dulos da API ServeRest:

### üë´ [/usuarios]

- **DoR (Definition of Ready)**
  - Banco de dados e infraestrutura para desenvolvimento disponibilizados.
  - Ambiente de testes configurado.

- **DoD (Definition of Done)**
  - CRUD de usu√°rios implementado (Criar, Atualizar, Listar, Deletar).
  - Testes cobrindo todos os verbos HTTP (GET, POST, PUT, DELETE).
  - Matriz de rastreabilidade atualizada.
  - Automa√ß√£o de testes baseada na an√°lise realizada.

 <img src="https://github.com/carolprotasio/ServerRest-Perfomance/blob/master/grafico-usuario.png"  width="600"/>

### üì¶ [/produtos]

- **DoR**
  - Ambiente de testes e banco de dados configurados.
  - Requisitos funcionais definidos.

- **DoD**
  - CRUD de produtos implementado.
  - Testes de valida√ß√£o de dados, incluindo verifica√ß√£o de campos obrigat√≥rios e formatos v√°lidos.
  - Automa√ß√£o de testes baseada na an√°lise realizada.
 
<img src="https://github.com/carolprotasio/ServerRest-Perfomance/blob/master/grafico-produto.png" width="600"/>  

### üõí [/carrinho]

- **DoR**
  - Banco de dados e API configurados.
  - Escopo de testes definido.

- **DoD**
  - CRUD de carrinhos implementado.
  - Valida√ß√£o de regras de neg√≥cio, como quantidade m√≠nima de itens e valida√ß√£o de usu√°rio autenticado.
  - Automa√ß√£o de testes com cobertura total de cen√°rios.
  - 
 <img src="https://github.com/carolprotasio/ServerRest-Perfomance/blob/master/grafico-carrinho.png" alt="Product Info" width="600"/>
  

## 4. Crit√©rios de Aceita√ß√£o dos M√≥dulos

### üë´ Usu√°rios

- Todos os usu√°rios devem possuir cadastro contendo: Nome, Email, Senha, e Administrador.
- A√ß√µes n√£o podem ser realizadas com usu√°rios inexistentes.
- N√£o deve ser poss√≠vel criar um usu√°rio com email j√° utilizado.
- Um novo usu√°rio deve ser criado caso o ID informado no PUT n√£o seja encontrado.
- Emails devem seguir um padr√£o v√°lido.

### üì¶ Produtos

- Produtos devem ter os campos: Nome, Pre√ßo, Descri√ß√£o, e Quantidade.
- N√£o deve ser poss√≠vel cadastrar produtos com nome duplicado.
- Produtos devem ser acess√≠veis apenas por usu√°rios autenticados.
- Produtos inativos n√£o devem aparecer na listagem p√∫blica.

### üõí Carrinho

- Carrinho deve estar associado a um usu√°rio autenticado.
- N√£o deve ser poss√≠vel adicionar produtos inexistentes ao carrinho.
- Quantidade de produtos no carrinho deve respeitar os limites estabelecidos (ex: estoque dispon√≠vel).
- Carrinho deve ser esvaziado ap√≥s a conclus√£o da compra.

## 5. üìä An√°lise dos Testes

### 5.1. M√≥dulo Usu√°rio

- **Comportamento Geral**: O m√≥dulo de usu√°rios apresentou tempos de resposta consistentes. O **GET** teve o melhor desempenho, com um tempo m√©dio de resposta em torno de **300 ms**, enquanto o **POST** foi o mais demorado, com uma m√©dia de **500 ms**. A maioria das requisi√ß√µes foi processada de forma eficiente, com o percentil 90 indicando que 90% das requisi√ß√µes foram atendidas dentro de **650 ms**.
- **Erros**: A taxa de erros foi baixa, relacionada principalmente a tentativas de criar usu√°rios com emails duplicados ou inexistentes, o que √© esperado e reflete a robustez das valida√ß√µes do sistema.
- **Conclus√£o**: O m√≥dulo de usu√°rios demonstrou bom desempenho e baixa taxa de erros, sendo capaz de lidar com m√∫ltiplas requisi√ß√µes simult√¢neas de maneira eficaz.

![usuario-teste](https://github.com/carolprotasio/ServerRest-Perfomance/blob/master/Crud-usuario-agregado.png)  

### 5.2. M√≥dulo Produto

- **Comportamento Geral**: O m√≥dulo de produtos seguiu uma tend√™ncia semelhante ao de usu√°rios, com o **GET** novamente sendo o mais r√°pido, e o **POST** e **PUT** apresentando tempos de resposta mais altos. O tempo m√©dio de resposta do **POST** foi de **520 ms**, enquanto o **GET** ficou em torno de **310 ms**. A maioria das requisi√ß√µes foi atendida dentro de **670 ms** (percentil 90).
- **Erros**: A taxa de erros foi relativamente baixa, com a maioria dos problemas surgindo de tentativas de cadastro de produtos com nomes ou IDs duplicados.
- **Conclus√£o**: O m√≥dulo de produtos demonstrou efici√™ncia, com tempos de resposta adequados e uma baixa incid√™ncia de erros, confirmando a capacidade do sistema em gerenciar opera√ß√µes complexas de produtos.


![produto-teste](https://github.com/carolprotasio/ServerRest-Perfomance/blob/master/crud-produto-agregado.png)  

### 5.3. M√≥dulo Carrinho

- **Comportamento Geral**: O m√≥dulo de carrinho apresentou um desempenho menos est√°vel em compara√ß√£o aos outros m√≥dulos, com tempos de resposta variando mais amplamente. O **PUT** e o **DELETE** tiveram tempos m√©dios de resposta de **550 ms** e **570 ms**, respectivamente, mas o **POST** apresentou uma m√©dia mais alta, em torno de **600 ms**. O percentil 90 mostrou que 90% das requisi√ß√µes foram conclu√≠das em at√© **710 ms**.
- **Erros**: A taxa de erros foi significativamente mais alta, chegando a quase **10%**, o que indica problemas durante as opera√ß√µes de **POST** e **PUT**. Esses erros estavam relacionados a falhas na cria√ß√£o e atualiza√ß√£o de carrinhos, possivelmente devido a problemas na l√≥gica de processamento ou concorr√™ncia no acesso aos dados.
- **Conclus√£o**: O m√≥dulo de carrinho demonstrou fragilidades, especialmente na cria√ß√£o e atualiza√ß√£o de carrinhos, refletido na alta taxa de erros. Isso sugere que o sistema pode precisar de otimiza√ß√µes para melhorar a confiabilidade e o desempenho nesse m√≥dulo espec√≠fico.

![carrinho-teste](https://github.com/carolprotasio/ServerRest-Perfomance/blob/master/crud-carrinho-agregado.png)
  
## 5.4. Conclus√£o dos testes de performance

Os testes de performance dos tr√™s m√≥dulos revelaram que, embora os m√≥dulos de usu√°rio e produto tenham mostrado bom desempenho com baixa taxa de erros, o m√≥dulo de carrinho apresentou problemas significativos, com uma taxa de erros de quase 10%. O **GET** foi o mais eficiente em todos os m√≥dulos, enquanto as opera√ß√µes de **POST** e **PUT** enfrentaram desafios maiores, especialmente no m√≥dulo de carrinho. A an√°lise indica que o ServeRest √© robusto, mas com √°reas que necessitam de melhorias, especialmente na gest√£o de carrinhos, para garantir uma performance consistente em ambientes de alta carga.

## 6. Como Executar o Projeto

### 6.1 Pr√©-requisitos

- Docker instalado na m√°quina.
- JMeter configurado (pode ser executado via Docker).

### 6.2 Clonando o Projeto

```bash
git clone https://github.com/carolprotasio/ServerRest-Perfomance.git
```
### 6.3 Executando os Testes
- Navegue at√© a pasta do projeto.
- Execute o comando Docker para iniciar o ambiente
- Abra o JMeter e carregue o arquivo ServeRest-Performance.jmx.
- Configure o n√∫mero de usu√°rios virtuais conforme desejado.

## 7. Conclus√£o do Projeto
Este projeto proporcionou um aprendizado valioso na aplica√ß√£o de testes de performance com JMeter e Docker, especialmente ao simular cen√°rios de uso com m√∫ltiplos usu√°rios acessando o sistema simultaneamente. A integra√ß√£o da biblioteca Faker no JSR223 Sampler tamb√©m se mostrou uma solu√ß√£o eficaz para a cria√ß√£o de dados din√¢micos, garantindo testes mais robustos e realistas.

O uso de Docker garantiu um ambiente de testes consistente e facilmente replic√°vel, essencial para manter a integridade dos resultados. Este projeto contribui significativamente para meu paprendizado, demonstrando habilidades avan√ßadas em testes de performance e automa√ß√£o de ambientes de teste.
