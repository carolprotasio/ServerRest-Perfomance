# ServeRest-Performance

## 1. Sobre o Projeto

Este projeto foi desenvolvido como parte dos meus estudos de QA e teste de performance, utilizando o JMeter, Docker, e a biblioteca Faker para gera√ß√£o de dados din√¢micos. O objetivo principal foi realizar testes de performance em diferentes m√≥dulos da API ServeRest (https://serverest.dev/), simulando o comportamento de m√∫ltiplos usu√°rios acessando e manipulando recursos simultaneamente.

Os testes foram realizados nos endpoints que cobrem as funcionalidades de usu√°rio, produto e carrinho, incluindo opera√ß√µes de CRUD (Criar, Ler, Atualizar, Deletar). Esse projeto permitiu aprofundar meus conhecimentos em automa√ß√£o de testes de performance e integra√ß√£o de ferramentas essenciais para a √°rea de QA.

### 1.1 JMeter

O JMeter √© uma ferramenta de c√≥digo aberto utilizada para testar a performance de aplica√ß√µes web. No contexto deste projeto, o JMeter foi empregado para simular m√∫ltiplos usu√°rios interagindo com a API do ServeRest, permitindo a an√°lise do desempenho da aplica√ß√£o sob diferentes cargas de trabalho.

### 1.2 Docker

O Docker foi utilizado para configurar e isolar o ambiente de testes, garantindo que o JMeter e todas as depend√™ncias necess√°rias estivessem corretamente configurados e reproduz√≠veis em qualquer ambiente. Com Docker, foi poss√≠vel garantir consist√™ncia e simplicidade na execu√ß√£o dos testes.
<img src="https://github.com/carolprotasio/ServerRest-Perfomance/blob/master/docker.png" alt="Product Info" width="600"/>

## 2. Tecnologias Utilizadas

- **JMeter**: Ferramenta para testes de carga e performance.
- **Docker**: Plataforma para virtualiza√ß√£o e automa√ß√£o de ambientes.
- **Faker**: Biblioteca para gera√ß√£o de dados fict√≠cios, usada na cria√ß√£o de cen√°rios de teste.
- **JSR223 Sampler**: Componente do JMeter usado para scripting avan√ßado, onde foi integrado o Faker.

## 3. Funcionalidades e M√≥dulos Testados

Foram testados os seguintes m√≥dulos da API ServeRest:

### üë´ [/usuarios]

- **DoR (Definition of Ready)**
  - Banco de dados e infraestrutura para desenvolvimento disponibilizados.
  - Ambiente de testes configurado.

- **DoD (Definition of Done)**
  - CRUD de usu√°rios implementado (Criar, Atualizar, Listar, Deletar).
  - Testes cobrindo todos os verbos HTTP (GET, POST, PUT, DELETE).
  - Matriz de rastreabilidade atualizada.
  - Automa√ß√£o de testes baseada na an√°lise realizada.

### üì¶ [/produtos]

- **DoR**
  - Ambiente de testes e banco de dados configurados.
  - Requisitos funcionais definidos.

- **DoD**
  - CRUD de produtos implementado.
  - Testes de valida√ß√£o de dados, incluindo verifica√ß√£o de campos obrigat√≥rios e formatos v√°lidos.
  - Automa√ß√£o de testes baseada na an√°lise realizada.

### üõí [/carrinho]

- **DoR**
  - Banco de dados e API configurados.
  - Escopo de testes definido.

- **DoD**
  - CRUD de carrinhos implementado.
  - Valida√ß√£o de regras de neg√≥cio, como quantidade m√≠nima de itens e valida√ß√£o de usu√°rio autenticado.
  - Automa√ß√£o de testes com cobertura total de cen√°rios.

## 4. Crit√©rios de Aceita√ß√£o dos M√≥dulos

### üë´ Usu√°rios

- Todos os usu√°rios devem possuir cadastro contendo: Nome, Email, Senha, e Administrador.
- A√ß√µes n√£o podem ser realizadas com usu√°rios inexistentes.
- N√£o deve ser poss√≠vel criar um usu√°rio com email j√° utilizado.
- Um novo usu√°rio deve ser criado caso o ID informado no PUT n√£o seja encontrado.
- Emails devem seguir um padr√£o v√°lido.

### üì¶ Produtos

- Produtos devem ter os campos: Nome, Pre√ßo, Descri√ß√£o, e Quantidade.
- N√£o deve ser poss√≠vel cadastrar produtos com nome duplicado.
- Produtos devem ser acess√≠veis apenas por usu√°rios autenticados.
- Produtos inativos n√£o devem aparecer na listagem p√∫blica.

### üõí Carrinho

- Carrinho deve estar associado a um usu√°rio autenticado.
- N√£o deve ser poss√≠vel adicionar produtos inexistentes ao carrinho.
- Quantidade de produtos no carrinho deve respeitar os limites estabelecidos (ex: estoque dispon√≠vel).
- Carrinho deve ser esvaziado ap√≥s a conclus√£o da compra.

## 5. Resultado dos Testes

(Inclua aqui as imagens dos resultados dos testes realizados, como gr√°ficos de tempo de resposta, taxa de sucesso, etc.)

## 6. Como Executar o Projeto

### 6.1 Pr√©-requisitos

- Docker instalado na m√°quina.
- JMeter configurado (pode ser executado via Docker).

### 6.2 Clonando o Projeto

```bash
git clone https://github.com/carolprotasio/ServerRest-Perfomance.git
```
### 6.3 Executando os Testes
- Navegue at√© a pasta do projeto.
- Execute o comando Docker para iniciar o ambiente
- Abra o JMeter e carregue o arquivo ServeRest-Performance.jmx.
- Configure o n√∫mero de usu√°rios virtuais conforme desejado.
- Execute o teste e visualize os resultados.

  ## 7. Conclus√£o do Projeto
Este projeto proporcionou um aprendizado valioso na aplica√ß√£o de testes de performance com JMeter e Docker, especialmente ao simular cen√°rios de uso com m√∫ltiplos usu√°rios acessando o sistema simultaneamente. A integra√ß√£o da biblioteca Faker no JSR223 Sampler tamb√©m se mostrou uma solu√ß√£o eficaz para a cria√ß√£o de dados din√¢micos, garantindo testes mais robustos e realistas.

O uso de Docker garantiu um ambiente de testes consistente e facilmente replic√°vel, essencial para manter a integridade dos resultados. Este projeto contribui significativamente para meu paprendizado, demonstrando habilidades avan√ßadas em testes de performance e automa√ß√£o de ambientes de teste.
